#!/usr/bin/env python3
"""
Alpha Robot å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¯åŠ¨è„šæœ¬
ç®€åŒ–ç‰ˆæœ¬ï¼Œé€‚åˆåˆå­¦è€…ä½¿ç”¨
"""

import os
import sys
import numpy as np
import torch
import matplotlib.pyplot as plt

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–åŒ…"""
    required_packages = [
        'pybullet', 'gymnasium', 'torch', 'numpy', 'matplotlib'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹ä¾èµ–åŒ…:")
        for package in missing_packages:
            print(f"   - {package}")
        print("\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    print("âœ… æ‰€æœ‰ä¾èµ–åŒ…å·²å®‰è£…")
    return True

def simple_train():
    """ç®€åŒ–çš„è®­ç»ƒå‡½æ•°"""
    print("=" * 50)
    print("ğŸ¤– Alpha Robot å¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return
    
    # å¯¼å…¥è®­ç»ƒæ¨¡å—ï¼ˆæ”¾åœ¨è¿™é‡Œé¿å…ä¾èµ–é—®é¢˜ï¼‰
    try:
        from envs.alpha_rl_env import AlphaRobotEnv, PPOTrainer
        print("âœ… æˆåŠŸå¯¼å…¥è®­ç»ƒæ¨¡å—")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥è®­ç»ƒæ¨¡å—å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿ alpha_rl_env.py æ–‡ä»¶å­˜åœ¨äºå½“å‰ç›®å½•")
        return
    
    # è®­ç»ƒé…ç½®
    config = {
        'render_mode': 'human',  # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
        'num_iterations': 100,   # è®­ç»ƒè¿­ä»£æ¬¡æ•°
        'steps_per_iteration': 512,  # æ¯æ¬¡è¿­ä»£çš„æ­¥æ•°
        'save_interval': 10,     # ä¿å­˜æ¨¡å‹çš„é—´éš”
        'test_interval': 5,      # æµ‹è¯•ç­–ç•¥çš„é—´éš”
    }
    
    print("ğŸ“‹ è®­ç»ƒé…ç½®:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    print()
    
    try:
        # åˆ›å»ºç¯å¢ƒ
        print("ğŸ—ï¸  åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
        env = AlphaRobotEnv(render_mode=config['render_mode'])
        
        # è·å–ç¯å¢ƒä¿¡æ¯
        state_size = env.observation_space.shape[0]
        action_size = env.action_space.shape[0]
        
        print(f"ğŸ“Š ç¯å¢ƒä¿¡æ¯:")
        print(f"   çŠ¶æ€ç©ºé—´å¤§å°: {state_size}")
        print(f"   åŠ¨ä½œç©ºé—´å¤§å°: {action_size}")
        print(f"   å¯ç§»åŠ¨å…³èŠ‚æ•°: {len(env.movable_joints)}")
        print()
        
        # åˆ›å»ºè®­ç»ƒå™¨
        print("ğŸ§  åˆ›å»ºPPOè®­ç»ƒå™¨...")
        trainer = PPOTrainer(env, state_size, action_size)
        
        # è®­ç»ƒå†å²
        rewards_history = []
        best_reward = -float('inf')
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        print("æç¤º: ä½ å¯ä»¥çœ‹åˆ°æœºå™¨äººåœ¨GUIä¸­å­¦ä¹ å¦‚ä½•è¾¾åˆ°çº¢è‰²ç›®æ ‡çƒ")
        print("æŒ‰ Ctrl+C å¯ä»¥éšæ—¶åœæ­¢è®­ç»ƒ")
        print()
        
        for iteration in range(config['num_iterations']):
            print(f"ğŸ“ˆ è¿­ä»£ {iteration + 1}/{config['num_iterations']}")
            
            # æ”¶é›†ç»éªŒ
            print("   æ”¶é›†ç»éªŒä¸­...")
            trainer.collect_experience(config['steps_per_iteration'])
            
            # è®­ç»ƒ
            print("   æ›´æ–°ç­–ç•¥ä¸­...")
            trainer.train()
            
            # æµ‹è¯•ç­–ç•¥
            if iteration % config['test_interval'] == 0:
                print("   æµ‹è¯•å½“å‰ç­–ç•¥...")
                test_reward = test_policy(env, trainer.policy)
                rewards_history.append(test_reward)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if test_reward > best_reward:
                    best_reward = test_reward
                    torch.save(trainer.policy.state_dict(), 'best_alpha_robot_model.pth')
                    print(f"   ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! å¥–åŠ±: {test_reward:.2f}")
                else:
                    print(f"   å½“å‰å¥–åŠ±: {test_reward:.2f} (æœ€ä½³: {best_reward:.2f})")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if iteration % config['save_interval'] == 0:
                torch.save(trainer.policy.state_dict(), f'alpha_robot_checkpoint_{iteration}.pth')
                print(f"   ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: alpha_robot_checkpoint_{iteration}.pth")
            
            print()
        
        print("ğŸŠ è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ä½³å¥–åŠ±: {best_reward:.2f}")
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        if rewards_history:
            plt.figure(figsize=(12, 6))
            plt.plot(rewards_history, 'b-', linewidth=2)
            plt.title('Alpha Robot Training Progress', fontsize=16)
            plt.xlabel('Iteration', fontsize=12)
            plt.ylabel('Test Reward', fontsize=12)
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig('alpha_robot_training_progress.png', dpi=300)
            print("ğŸ“Š è®­ç»ƒæ›²çº¿å·²ä¿å­˜ä¸º: alpha_robot_training_progress.png")
            plt.show()
        
        # æœ€ç»ˆæµ‹è¯•
        print("\nğŸ¯ æœ€ç»ˆæµ‹è¯•...")
        final_reward = test_policy(env, trainer.policy, num_episodes=10)
        print(f"æœ€ç»ˆå¹³å‡å¥–åŠ±: {final_reward:.2f}")
        
        env.close()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        if 'env' in locals():
            env.close()
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        if 'env' in locals():
            env.close()

def test_policy(env, policy, num_episodes=3):
    """æµ‹è¯•ç­–ç•¥æ€§èƒ½"""
    total_reward = 0
    
    for episode in range(num_episodes):
        try:
            state, _ = env.reset()
        except:
            state = np.zeros(env.observation_space.shape[0])
        
        episode_reward = 0
        done = False
        steps = 0
        
        while not done and steps < 1000:  # é™åˆ¶æœ€å¤§æ­¥æ•°
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            
            with torch.no_grad():
                action_mean, _, _ = policy(state_tensor)
            
            action = action_mean.cpu().numpy()[0]
            
            try:
                state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                episode_reward += reward
                steps += 1
            except:
                done = True
                break
        
        total_reward += episode_reward
    
    return total_reward / num_episodes

def load_and_test_model():
    """åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹å¹¶æµ‹è¯•"""
    print("ğŸ”„ åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæµ‹è¯•...")
    
    # å¯¼å…¥æ‰€éœ€æ¨¡å—
    try:
        from envs.alpha_rl_env import AlphaRobotEnv, ActorCritic
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        return
    
    # æŸ¥æ‰¾å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶
    model_files = []
    for file in os.listdir('.'):
        if file.endswith('.pth') and 'alpha_robot' in file:
            model_files.append(file)
    
    if not model_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹æ–‡ä»¶")
        return
    
    # é€‰æ‹©æ¨¡å‹
    if 'best_alpha_robot_model.pth' in model_files:
        model_path = 'best_alpha_robot_model.pth'
    else:
        model_path = model_files[0]
    
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
    
    try:
        # åˆ›å»ºç¯å¢ƒå’Œç­–ç•¥
        env = AlphaRobotEnv(render_mode='human')
        state_size = env.observation_space.shape[0]
        action_size = env.action_space.shape[0]
        
        policy = ActorCritic(state_size, action_size)
        policy.load_state_dict(torch.load(model_path))
        policy.eval()
        
        # æµ‹è¯•æ¨¡å‹
        print("ğŸ® æµ‹è¯•æ¨¡å‹æ€§èƒ½...")
        test_reward = test_policy(env, policy, num_episodes=5)
        print(f"å¹³å‡å¥–åŠ±: {test_reward:.2f}")
        
        env.close()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("ğŸ¤– Alpha Robot å¼ºåŒ–å­¦ä¹ è®­ç»ƒè„šæœ¬")
    print()
    print("ç”¨æ³•:")
    print("  python simple_train_script.py        # å¼€å§‹è®­ç»ƒ")
    print("  python simple_train_script.py test   # æµ‹è¯•å·²è®­ç»ƒçš„æ¨¡å‹")
    print("  python simple_train_script.py help   # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
    print()
    print("æ–‡ä»¶è¯´æ˜:")
    print("  - alpha_rl_env.py: ç¯å¢ƒå’Œç®—æ³•å®ç°")
    print("  - simple_train_script.py: è®­ç»ƒå¯åŠ¨è„šæœ¬ï¼ˆæ¨èä½¿ç”¨ï¼‰")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] == 'test':
            load_and_test_model()
        elif sys.argv[1] == 'help':
            show_help()
        else:
            print("âŒ æœªçŸ¥å‚æ•°ï¼Œä½¿ç”¨ 'help' æŸ¥çœ‹å¸®åŠ©")
    else:
        simple_train()