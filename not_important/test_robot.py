#!/usr/bin/env python3
"""
Alpha Robot è®­ç»ƒç»“æœæµ‹è¯•è„šæœ¬
ä¸“é—¨ç”¨æ¥è§‚å¯Ÿè®­ç»ƒå¥½çš„æœºå™¨äººè¡Œä¸º
"""

import os
import sys
import numpy as np
import torch
import time
import pybullet as p
from envs.alpha_rl_env import AlphaRobotEnv, ActorCritic

def find_best_model():
    """æŸ¥æ‰¾æœ€ä½³æ¨¡å‹æ–‡ä»¶"""
    model_files = []
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹æ–‡ä»¶
    possible_files = [
        'best_alpha_robot_model.pth',
        'alpha_robot_model_90.pth',
        'alpha_robot_model_80.pth',
        'alpha_robot_model_70.pth',
        'alpha_robot_model_60.pth',
        'alpha_robot_model_50.pth',
    ]
    
    for file in possible_files:
        if os.path.exists(file):
            model_files.append(file)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¢„å®šä¹‰çš„æ–‡ä»¶ï¼Œæœç´¢æ‰€æœ‰.pthæ–‡ä»¶
    if not model_files:
        for file in os.listdir('.'):
            if file.endswith('.pth') and 'alpha_robot' in file:
                model_files.append(file)
    
    if not model_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶")
        print("è¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨ï¼š")
        print("   - best_alpha_robot_model.pth")
        print("   - alpha_robot_model_XX.pth")
        return None
    
    # ä¼˜å…ˆé€‰æ‹©æœ€ä½³æ¨¡å‹
    if 'best_alpha_robot_model.pth' in model_files:
        return 'best_alpha_robot_model.pth'
    else:
        return model_files[0]

def test_trained_robot(model_path=None, num_episodes=5, slow_motion=True):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æœºå™¨äºº"""
    
    print("ğŸ¤– Alpha Robot è¡Œä¸ºæµ‹è¯•")
    print("=" * 40)
    
    # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
    if model_path is None:
        model_path = find_best_model()
        if model_path is None:
            return
    
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
    
    # åˆ›å»ºç¯å¢ƒï¼ˆGUIæ¨¡å¼ï¼Œå¯ä»¥çœ‹åˆ°æœºå™¨äººï¼‰
    print("ğŸ—ï¸  åˆ›å»ºæµ‹è¯•ç¯å¢ƒ...")
    env = AlphaRobotEnv(render_mode='human')
    
    # è·å–ç¯å¢ƒä¿¡æ¯
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.shape[0]
    
    print(f"ğŸ“Š ç¯å¢ƒä¿¡æ¯:")
    print(f"   çŠ¶æ€ç©ºé—´å¤§å°: {state_size}")
    print(f"   åŠ¨ä½œç©ºé—´å¤§å°: {action_size}")
    print(f"   å¯ç§»åŠ¨å…³èŠ‚æ•°: {len(env.movable_joints)}")
    
    # åˆ›å»ºç­–ç•¥ç½‘ç»œ
    policy = ActorCritic(state_size, action_size)
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    try:
        policy.load_state_dict(torch.load(model_path, map_location='cpu'))
        policy.eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        env.close()
        return
    
    print(f"\nğŸ® å¼€å§‹æµ‹è¯• {num_episodes} ä¸ªå›åˆ...")
    print("ğŸ¯ è§‚å¯Ÿæœºå™¨äººå¦‚ä½•ç§»åŠ¨å»è§¦ç¢°çº¢è‰²ç›®æ ‡çƒ")
    if slow_motion:
        print("ğŸŒ æ…¢é€Ÿæ¨¡å¼ï¼šä¾¿äºè§‚å¯Ÿæœºå™¨äººè¡Œä¸º")
    print("æŒ‰ Ctrl+C å¯ä»¥éšæ—¶åœæ­¢æµ‹è¯•\n")
    
    total_rewards = []
    success_count = 0
    
    try:
        for episode in range(num_episodes):
            print(f"ğŸ“ å›åˆ {episode + 1}/{num_episodes}")
            
            # é‡ç½®ç¯å¢ƒ
            state, _ = env.reset()
            episode_reward = 0
            steps = 0
            max_steps = 1000
            
            print(f"   ğŸ¯ ç›®æ ‡ä½ç½®: ({env.target_position[0]:.2f}, {env.target_position[1]:.2f}, {env.target_position[2]:.2f})")
            
            while steps < max_steps:
                # è·å–åŠ¨ä½œ
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                
                with torch.no_grad():
                    action_mean, action_std, state_value = policy(state_tensor)
                
                # ä½¿ç”¨ç¡®å®šæ€§åŠ¨ä½œï¼ˆå‡å€¼ï¼‰æ¥è§‚å¯Ÿå­¦åˆ°çš„è¡Œä¸º
                action = action_mean.cpu().numpy()[0]
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                
                episode_reward += reward
                steps += 1
                
                # æ¯50æ­¥æ˜¾ç¤ºä¸€æ¬¡ä¿¡æ¯
                if steps % 50 == 0:
                    # è·å–æœºå™¨äººå½“å‰ä½ç½®
                    num_links = p.getNumJoints(env.robot_id)
                    if num_links > 0:
                        try:
                            end_effector_state = p.getLinkState(env.robot_id, num_links - 1)
                            end_pos = end_effector_state[0]
                        except:
                            end_pos = p.getBasePositionAndOrientation(env.robot_id)[0]
                    else:
                        end_pos = p.getBasePositionAndOrientation(env.robot_id)[0]
                    
                    distance = np.linalg.norm(np.array(end_pos) - env.target_position)
                    print(f"   æ­¥æ•°: {steps:3d}, è·ç¦»: {distance:.3f}, å¥–åŠ±: {reward:.2f}")
                
                # æ…¢é€Ÿæ¨¡å¼
                if slow_motion:
                    time.sleep(0.05)  # 50mså»¶è¿Ÿï¼Œä¾¿äºè§‚å¯Ÿ
                
                if done:
                    if terminated:
                        print(f"   ğŸ‰ æˆåŠŸï¼æœºå™¨äººè¾¾åˆ°äº†ç›®æ ‡ä½ç½®")
                        success_count += 1
                    else:
                        print(f"   â° å›åˆç»“æŸï¼ˆè¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼‰")
                    break
                
                state = next_state
            
            total_rewards.append(episode_reward)
            print(f"   å›åˆå¥–åŠ±: {episode_reward:.2f}")
            print(f"   æ­¥æ•°: {steps}")
            print()
            
            # å›åˆé—´æš‚åœ
            if episode < num_episodes - 1:
                print("   ç­‰å¾…3ç§’åå¼€å§‹ä¸‹ä¸€å›åˆ...")
                time.sleep(3)
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    
    # ç»Ÿè®¡ç»“æœ
    print("ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:")
    print(f"   æ€»å›åˆæ•°: {len(total_rewards)}")
    print(f"   æˆåŠŸæ¬¡æ•°: {success_count}")
    print(f"   æˆåŠŸç‡: {success_count/len(total_rewards)*100:.1f}%")
    print(f"   å¹³å‡å¥–åŠ±: {np.mean(total_rewards):.2f}")
    print(f"   æœ€ä½³å¥–åŠ±: {np.max(total_rewards):.2f}")
    print(f"   æœ€å·®å¥–åŠ±: {np.min(total_rewards):.2f}")
    
    env.close()

def interactive_test():
    """äº¤äº’å¼æµ‹è¯•æ¨¡å¼"""
    print("ğŸ® äº¤äº’å¼æµ‹è¯•æ¨¡å¼")
    print("=" * 40)
    
    # æŸ¥æ‰¾æ¨¡å‹
    model_path = find_best_model()
    if model_path is None:
        return
    
    while True:
        print("\nè¯·é€‰æ‹©æµ‹è¯•é€‰é¡¹ï¼š")
        print("1. æ…¢é€Ÿè§‚å¯Ÿ (æ¨è)")
        print("2. æ­£å¸¸é€Ÿåº¦")
        print("3. å•å›åˆè¯¦ç»†æµ‹è¯•")
        print("4. è¿ç»­æµ‹è¯•å¤šå›åˆ")
        print("5. é€€å‡º")
        
        choice = input("è¾“å…¥é€‰é¡¹ (1-5): ").strip()
        
        if choice == '1':
            test_trained_robot(model_path, num_episodes=3, slow_motion=True)
        elif choice == '2':
            test_trained_robot(model_path, num_episodes=3, slow_motion=False)
        elif choice == '3':
            test_trained_robot(model_path, num_episodes=1, slow_motion=True)
        elif choice == '4':
            num = input("è¾“å…¥å›åˆæ•° (é»˜è®¤5): ").strip()
            try:
                num_episodes = int(num) if num else 5
            except ValueError:
                num_episodes = 5
            test_trained_robot(model_path, num_episodes=num_episodes, slow_motion=False)
        elif choice == '5':
            print("ğŸ‘‹ å†è§ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")

def analyze_model_behavior():
    """åˆ†ææ¨¡å‹è¡Œä¸º"""
    print("ğŸ” æ¨¡å‹è¡Œä¸ºåˆ†æ")
    print("=" * 40)
    
    model_path = find_best_model()
    if model_path is None:
        return
    
    # åˆ›å»ºç¯å¢ƒ
    env = AlphaRobotEnv(render_mode='human')
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.shape[0]
    
    # åŠ è½½æ¨¡å‹
    policy = ActorCritic(state_size, action_size)
    policy.load_state_dict(torch.load(model_path, map_location='cpu'))
    policy.eval()
    
    print("ğŸ§  åˆ†ææ¨¡å‹å¯¹ä¸åŒçŠ¶æ€çš„ååº”...")
    
    # æµ‹è¯•ä¸åŒçš„ç›®æ ‡ä½ç½®
    test_targets = [
        [0.1, 0.0, 0.1],   # è¿‘è·ç¦»
        [0.2, 0.0, 0.2],   # ä¸­è·ç¦»
        [0.2, 0.1, 0.1],   # ä¾§è¾¹
        [0.2, -0.1, 0.2],  # å¦ä¸€ä¾§
    ]
    
    for i, target in enumerate(test_targets):
        print(f"\nğŸ“ æµ‹è¯•ç›®æ ‡ä½ç½® {i+1}: {target}")
        
        # æ‰‹åŠ¨è®¾ç½®ç›®æ ‡ä½ç½®
        env.target_position = np.array(target)
        p.resetBasePositionAndOrientation(env.target_id, target, [0, 0, 0, 1])
        1
        state, _ = env.reset()
        
        # è§‚å¯Ÿå‰å‡ æ­¥çš„åŠ¨ä½œ
        for step in range(10):
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            with torch.no_grad():
                action_mean, action_std, state_value = policy(state_tensor)
            
            action = action_mean.cpu().numpy()[0]
            print(f"   æ­¥æ•° {step+1}: åŠ¨ä½œ = {action[:3]}, ä»·å€¼ = {state_value.item():.3f}")
            
            state, reward, terminated, truncated, _ = env.step(action)
            if terminated or truncated:
                break
            
            time.sleep(0.1)
    
    env.close()

if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] == 'analyze':
            analyze_model_behavior()
        elif sys.argv[1] == 'quick':
            test_trained_robot(num_episodes=1, slow_motion=True)
        else:
            print("ç”¨æ³•:")
            print("  python test_robot.py          # äº¤äº’å¼æµ‹è¯•")
            print("  python test_robot.py quick    # å¿«é€Ÿæµ‹è¯•ä¸€å›åˆ")
            print("  python test_robot.py analyze  # åˆ†ææ¨¡å‹è¡Œä¸º")
    else:
        interactive_test()