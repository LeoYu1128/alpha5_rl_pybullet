import pybullet as p
import pybullet_data
import os
import numpy as np
import gymnasium as gym
from gymnasium import spaces
import yaml
from envs.alpha_controller import AlphaRobotController

class AlphaRobotEnv(gym.Env):
    """Alpha Robot å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ"""
    
    def __init__(self, render_mode="human", max_steps=1000, dense_reward=True,
                 enable_safety=True, curriculum_learning=False):
        super(AlphaRobotEnv, self).__init__()

        # ğŸ†• ç®€å•çš„ç›®æ ‡ç®¡ç†
        self.target_list = [
            np.array([0.2, 0.1, 0.2]),      # ç›®æ ‡1ï¼šæ­£å‰æ–¹
            np.array([0.15, 0.15, 0.25]),   # ç›®æ ‡2ï¼šå³å‰æ–¹  
            np.array([0.25, -0.15, 0.2]),   # ç›®æ ‡3ï¼šå·¦å‰æ–¹
            np.array([0.2, 0.1, 0.2]),      # ç›®æ ‡4ï¼šè¿œé«˜ä½
            np.array([0.18, 0.0, 0.15]),    # ç›®æ ‡5ï¼šè¿‘ä½ä½
        ] 
        self.current_target_index = 0
        self.success_count = 0
        self.episodes_on_target = 0
        self.required_successes = 5  # è¿ç»­æˆåŠŸ5æ¬¡æ‰æ¢ç›®æ ‡
        self.prev_distance = None
        self.realistic_controller = None  # åˆå§‹åŒ–æ§åˆ¶å™¨
        self.use_realistic_controller = True  # æ˜¯å¦ä½¿ç”¨çœŸå®æ§åˆ¶å™¨
        self.render_mode = render_mode
        self.max_steps = max_steps
        self.current_step = 0
        self.dense_reward = dense_reward
        self.enable_safety = enable_safety
        self.curriculum_learning = curriculum_learning
        
        # Alphaæœºæ¢°è‡‚å‚æ•°ï¼ˆåŸºäºYAMLé…ç½®ï¼‰
        self.MAX_REACH = 0.4  # æœ€å¤§å¯è¾¾èŒƒå›´ 400mm
        self.MIN_REACH = 0.08  # æœ€å°å¯è¾¾èŒƒå›´ï¼ˆé¿å…å¥‡å¼‚æ€§ï¼‰
        self.GRIPPER_RANGE = 0.012  # å¤¹çˆªèŒƒå›´ (0.0133 - 0.0013)
        
        # å…³èŠ‚é™åˆ¶ï¼ˆæ¥è‡ªalpha_joint_lim_urdf.yamlï¼‰
        self.joint_limits = {
            'joint_1': {'lower': 0.032, 'upper': 6.02, 'effort': 54.36, 'velocity': 2.0},
            'joint_2': {'lower': 0.0174533, 'upper': 3.40339, 'effort': 54.36, 'velocity': 2.0},
            'joint_3': {'lower': 0.0174533, 'upper': 3.40339, 'effort': 47.112, 'velocity': 2.0},
            'joint_4': {'lower': -3.14159, 'upper': 3.14159, 'effort': 33.069, 'velocity': 2.0},
            'joint_5': {'lower': 0.0013, 'upper': 0.0133, 'effort': 28.992, 'velocity': 1.0}
        }
        
        # å®‰å…¨å‚æ•°
        self.joint_safety_margin = 0.05  # å…³èŠ‚é™ä½å®‰å…¨è¾¹è·ï¼ˆå¼§åº¦ï¼‰
        self.collision_threshold = 0.02  # ç¢°æ’æ£€æµ‹é˜ˆå€¼ï¼ˆç±³ï¼‰
        self.singularity_threshold = 0.05  # å¥‡å¼‚æ€§æ£€æµ‹é˜ˆå€¼
        self.max_joint_acceleration = 5.0  # æœ€å¤§å…³èŠ‚åŠ é€Ÿåº¦
        
        # åˆå§‹å…³èŠ‚ä½ç½®ï¼ˆå®‰å…¨çš„ä¸­é—´ä½ç½®ï¼‰
        self.initial_joint_positions = [
            3.0,    # joint_1: åŸºåº§æ—‹è½¬ï¼ˆä¸­é—´ä½ç½®ï¼‰
            3.0,    # joint_2: è‚©éƒ¨ï¼ˆç¨å¾®æŠ¬èµ·ï¼‰
            1.0,    # joint_3: è‚˜éƒ¨ï¼ˆå¼¯æ›²ï¼‰
            0.0,    # joint_4: è…•éƒ¨æ—‹è½¬ï¼ˆä¸­é—´ï¼‰
            0.01,   # joint_5: å¤¹çˆªï¼ˆå¾®å¼€ï¼‰
        ]
        
        # è¿æ¥PyBullet
        if self.render_mode == "human":
            self.physics_client = p.connect(p.GUI)
            p.configureDebugVisualizer(p.COV_ENABLE_GUI, 1)
            p.configureDebugVisualizer(p.COV_ENABLE_SHADOWS, 1)
            p.configureDebugVisualizer(p.COV_ENABLE_RGB_BUFFER_PREVIEW, 0)
            p.configureDebugVisualizer(p.COV_ENABLE_DEPTH_BUFFER_PREVIEW, 0)
            p.configureDebugVisualizer(p.COV_ENABLE_SEGMENTATION_MARK_PREVIEW, 0)
            p.resetDebugVisualizerCamera(
                cameraDistance=0.8,
                cameraYaw=45,
                cameraPitch=-30,
                cameraTargetPosition=[0, 0, 0.2]
            )
        else:
            self.physics_client = p.connect(p.DIRECT)
            
        p.setGravity(0, 0, -9.81)
        p.setTimeStep(1./240.)
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        
        # åˆå§‹åŒ–åœºæ™¯
        self._setup_scene()
        
        # å®šä¹‰åŠ¨ä½œç©ºé—´ï¼ˆ5ä¸ªå…³èŠ‚çš„å½’ä¸€åŒ–åŠ¨ä½œï¼‰
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, 
            shape=(5,), 
            dtype=np.float32
        )
        
        # å®šä¹‰è§‚å¯Ÿç©ºé—´
        # [å…³èŠ‚ä½ç½®(5), å…³èŠ‚é€Ÿåº¦(5), æœ«ç«¯ä½ç½®(3), æœ«ç«¯å§¿æ€(4), 
        #  ç›®æ ‡ä½ç½®(3), ç›¸å¯¹ä½ç½®(3), é›…å¯æ¯”æ¡ä»¶æ•°(1), å…³èŠ‚æ‰­çŸ©(5)]
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf,
            #shape=(29,), 
            shape=(16,), 
            dtype=np.float32
        )
        
        # æ€§èƒ½è¿½è¸ª
        self.episode_rewards = []
        self.success_count = 0
        self.collision_count = 0
        self.singularity_count = 0
        
        # è¯¾ç¨‹å­¦ä¹ å‚æ•°
        if self.curriculum_learning:
            self.difficulty_level = 0.0  # 0åˆ°1ï¼Œé€æ¸å¢åŠ éš¾åº¦
            self.success_threshold = 0.8  # æˆåŠŸç‡é˜ˆå€¼ï¼Œç”¨äºæå‡éš¾åº¦
            self.recent_success_rate = 0.0
            
    def _setup_scene(self):
        """è®¾ç½®åœºæ™¯"""
        # åˆ›å»ºåœ°é¢
        self.plane_id = p.loadURDF("plane.urdf")
        
        # è®¾ç½®åœ°é¢æ‘©æ“¦åŠ›
        p.changeDynamics(self.plane_id, -1, 
                        lateralFriction=1.0,
                        spinningFriction=0.1,
                        rollingFriction=0.1)
        
        # åŠ è½½æœºå™¨äºº
        self.robot_id = self._load_robot()
        #self.table = p.loadURDF("table/table.urdf", basePosition=[0, 0, 0])
        # è·å–å…³èŠ‚ä¿¡æ¯
        self._setup_joints()
        
        # åˆ›å»ºç›®æ ‡
        self._create_target()
        
        # åˆ›å»ºå·¥ä½œç©ºé—´å¯è§†åŒ–
        if self.render_mode == "human":
            self._create_workspace_visualization()
            
        # åˆå§‹åŒ–ç¢°æ’æ£€æµ‹
        if self.enable_safety:
            self._setup_collision_detection()

        if self.use_realistic_controller:
            # åˆå§‹åŒ–çœŸå®æ§åˆ¶å™¨
            self.realistic_controller = AlphaRobotController(self.robot_id, self.joint_indices,self.joint_names)
            print("å·²ä½¿ç”¨ä¸¥è°¨çš„æ§åˆ¶å™¨")
            
    def _load_robot(self):
        
        """åŠ è½½æœºå™¨äººURDF"""
        robot_path = os.path.join(os.path.dirname(__file__), 
                                 "../alpha_description/urdf/alpha_robot_for_pybullet.urdf")
            
        robot_id = p.loadURDF(
            robot_path,
            basePosition=[0, 0, 0.02],
            useFixedBase=True,
            flags=p.URDF_USE_SELF_COLLISION | p.URDF_USE_SELF_COLLISION_EXCLUDE_PARENT
        )
        
        # è®¾ç½®å…³èŠ‚é˜»å°¼
        for i in range(p.getNumJoints(robot_id)):
            # p.changeDynamics(robot_id, i, 
            #                jointDamping=0.1, # é™†åœ°ä¸Šçš„é˜»å°¼
            #                lateralFriction=0.8)
            
            # æ¨¡æ‹Ÿæ°´ä¸‹è¡Œä¸º
            p.changeDynamics(robot_id, i,
                 jointDamping=0.7,  # å…³èŠ‚é˜»å°¼
                 lateralFriction=0.5,  # ä¾§å‘æ‘©æ“¦
                 linearDamping=0.1,  # ç©ºé—´é˜»å°¼
                 angularDamping=0.1)

                           
        return robot_id
        
    def _setup_joints(self):
        """è®¾ç½®å…³èŠ‚ä¿¡æ¯"""
        self.joint_indices = []
        self.joint_names = ['joint_1', 'joint_2', 'joint_3', 'joint_4', 'joint_5']
        self.joint_info = {}
        
        for i in range(p.getNumJoints(self.robot_id)):
            joint_info = p.getJointInfo(self.robot_id, i)
            joint_name = joint_info[1].decode('utf-8')
            
            if joint_name in self.joint_names:
                self.joint_indices.append(i)
                self.joint_info[joint_name] = {
                    'index': i,
                    'type': joint_info[2],
                    'lower': joint_info[8],
                    'upper': joint_info[9],
                    'max_force': joint_info[10],
                    'max_velocity': joint_info[11]
                }
                
        # æŸ¥æ‰¾æœ«ç«¯æ‰§è¡Œå™¨
        self.end_effector_index = self.joint_indices[-1] if self.joint_indices else 0
        
        # æŸ¥æ‰¾TCPï¼ˆå·¥å…·ä¸­å¿ƒç‚¹ï¼‰
        for i in range(p.getNumJoints(self.robot_id)):
            link_info = p.getJointInfo(self.robot_id, i)
            if b'tcp' in link_info[12].lower() or b'ee' in link_info[12].lower():
                self.tcp_index = i
                break
        else:
            self.tcp_index = self.end_effector_index
            
    def _create_target(self):
        """åˆ›å»ºç›®æ ‡"""
        # ç›®æ ‡çƒä½“
        self.target_visual = p.createVisualShape(
            p.GEOM_SPHERE, 
            radius=0.01, 
            rgbaColor=[1, 0, 0, 0.8]
        )
        
        self.target_collision = p.createCollisionShape(
            p.GEOM_SPHERE,
            radius=0.02
        )
        
        self.target_id = p.createMultiBody(
            baseMass=0,
            baseVisualShapeIndex=self.target_visual,
            baseCollisionShapeIndex=self.target_collision,
            basePosition=[0.2, 0.1, 0.2]
        )
        
    def _create_workspace_visualization(self):
        """åˆ›å»ºå·¥ä½œç©ºé—´å¯è§†åŒ–"""
        # å¯è¾¾èŒƒå›´çƒä½“
        workspace_visual = p.createVisualShape(
            p.GEOM_SPHERE,
            radius=self.MAX_REACH,
            rgbaColor=[0.2, 0.2, 0.8, 0.1]
        )
        
        p.createMultiBody(
            baseMass=0,
            baseVisualShapeIndex=workspace_visual,
            basePosition=[0, 0, 0.1]
        )
        
        # å®‰å…¨è¾¹ç•Œ
        safety_visual = p.createVisualShape(
            p.GEOM_SPHERE,
            radius=self.MIN_REACH,
            rgbaColor=[0.8, 0.2, 0.2, 0.1]
        )
        
        p.createMultiBody(
            baseMass=0,
            baseVisualShapeIndex=safety_visual,
            basePosition=[0, 0, 0.1]
        )
        
    def _setup_collision_detection(self):
        """è®¾ç½®ç¢°æ’æ£€æµ‹"""
        # è·å–æ‰€æœ‰é“¾æ¥å¯¹ï¼Œç”¨äºè‡ªç¢°æ’æ£€æµ‹
        self.link_pairs = []
        num_links = p.getNumJoints(self.robot_id)
        
        for i in range(num_links):
            for j in range(i + 2, num_links):  # è·³è¿‡ç›¸é‚»é“¾æ¥
                self.link_pairs.append((i, j))
                
    def reset(self, seed=None, options=None):
        """é‡ç½®ç¯å¢ƒ"""
        super().reset(seed=seed)   
        self.current_step = 0
        self.episode_rewards = []
        self.episodes_on_target += 1      
        # é‡ç½®æœºå™¨äººåˆ°å®‰å…¨åˆå§‹ä½ç½®
        for i, (joint_idx, pos) in enumerate(zip(self.joint_indices, self.initial_joint_positions)):
            p.resetJointState(self.robot_id, joint_idx, pos, targetVelocity=0)
            
        # ğŸ†• ä½¿ç”¨å½“å‰å›ºå®šç›®æ ‡
        self.target_position = self.target_list[self.current_target_index].copy()
        # # è®¾ç½®ç›®æ ‡ä½ç½®
        # if self.curriculum_learning:
        #     self.target_position = self._sample_curriculum_target()
        # else:
        #     self.target_position = self._sample_valid_target()
        
        p.resetBasePositionAndOrientation(
            self.target_id,
            self.target_position,
            [0, 0, 0, 1]
        )
         # ğŸ†• æ‰“å°è¿›åº¦ï¼ˆå¯é€‰ï¼‰
        if self.episodes_on_target % 50 == 0:
            print(f"ç›®æ ‡ {self.current_target_index + 1}/{len(self.target_list)}: {self.target_position}, "
                f"å·²è®­ç»ƒ {self.episodes_on_target} episodes, è¿ç»­æˆåŠŸ {self.success_count} æ¬¡")
            # é‡ç½®å†å²æ•°æ®
            self.prev_distance = self._get_distance_to_target()
            self.prev_joint_positions = np.array(self.initial_joint_positions)
        # self.prev_jacobian_cond = self._compute_jacobian_condition()
        
        # è¿è¡Œå‡ æ­¥ä»¿çœŸä»¥ç¨³å®š
        for _ in range(10):
            p.stepSimulation()
            
        return self._get_observation(), {}
        
    def _sample_valid_target(self):
        """é‡‡æ ·æœ‰æ•ˆçš„ç›®æ ‡ä½ç½®"""
        max_attempts = 100
        
        for _ in range(max_attempts):
            # åœ¨å·¥ä½œç©ºé—´å†…éšæœºé‡‡æ ·
            theta = np.random.uniform(0, 2 * np.pi)
            phi = np.random.uniform(np.pi/6, np.pi/2)  # é™åˆ¶ä¿¯ä»°è§’
            r = np.random.uniform(self.MIN_REACH + 0.05, self.MAX_REACH - 0.05)
            
            x = r * np.sin(phi) * np.cos(theta)
            y = r * np.sin(phi) * np.sin(theta)
            z = r * np.cos(phi) + 0.05  # åŸºåº§é«˜åº¦åç§»
            
            # ç¡®ä¿ç›®æ ‡åœ¨åˆç†é«˜åº¦
            if 0.05 < z < 0.35:
                target = np.array([x, y, z])
                
                # æ£€æŸ¥æ˜¯å¦å¯è¾¾ï¼ˆç®€å•é€†è¿åŠ¨å­¦æ£€æŸ¥ï¼‰
                if self._is_position_reachable(target):
                    return target
                    
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›é»˜è®¤å®‰å…¨ä½ç½®
        return np.array([0.2, 0.0, 0.2])
        
    def _sample_curriculum_target(self):
        """è¯¾ç¨‹å­¦ä¹ çš„ç›®æ ‡é‡‡æ ·"""
        # æ ¹æ®éš¾åº¦è°ƒæ•´ç›®æ ‡èŒƒå›´
        min_r = self.MIN_REACH + 0.05
        max_r = min_r + (self.MAX_REACH - min_r) * (0.5 + 0.5 * self.difficulty_level)
        
        # è§’åº¦èŒƒå›´ä¹Ÿéšéš¾åº¦å¢åŠ 
        max_theta = np.pi * (0.5 + 0.5 * self.difficulty_level)
        
        theta = np.random.uniform(-max_theta, max_theta)
        phi = np.random.uniform(np.pi/4, np.pi/2)
        r = np.random.uniform(min_r, max_r)
        
        x = r * np.sin(phi) * np.cos(theta)
        y = r * np.sin(phi) * np.sin(theta)
        z = r * np.cos(phi) + 0.05
        
        return np.array([x, y, z])
        
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        self.current_step += 1
        
        # # åº”ç”¨å®‰å…¨çº¦æŸ
        # if self.enable_safety:
        #     if self.current_step < self.max_steps // 2:
        #         action = self._apply_safety_constraints_easy(action)
        #     else:
        #         action = self._apply_safety_constraints_hard(action)
        
        # ç®€å•ç›´æ¥çš„çº¦æŸ
        action = np.clip(action, -1.0, 1.0)  # ç¡®ä¿åœ¨åŠ¨ä½œç©ºé—´å†…
        # å°†å½’ä¸€åŒ–åŠ¨ä½œè½¬æ¢ä¸ºå…³èŠ‚ç›®æ ‡
        # target_positions = self._action_to_joint_positions(action)

        # å°†å½’ä¸€åŒ–çš„åŠ¨ä½œæ˜ å°„åˆ°æ‰­çŸ©èŒƒå›´
        torques = self._action_to_torques(action)  # ä¸æ˜¯ positionsï¼
        # åº”ç”¨å…³èŠ‚æ§åˆ¶
        # self._apply_joint_control(target_positions)

        # åº”ç”¨å…³èŠ‚æ‰­çŸ©æ§åˆ¶
        self._apply_torque_control(torques)
        # æ­¥è¿›ä»¿çœŸ
        for _ in range(4):
            p.stepSimulation()
            if self.render_mode == "human":
                pass  # GUIè‡ªåŠ¨æ¸²æŸ“
                
        # æ£€æµ‹å®‰å…¨è¿è§„
        safety_penalty = 0.0
        if self.enable_safety:
            safety_penalty = self._check_safety()
            
        # è·å–æ–°çŠ¶æ€
        observation = self._get_observation()
        
        # è®¡ç®—å¥–åŠ±
        reward = self._calculate_reward() - safety_penalty
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        success = self._is_success()  # ğŸ†• åªè°ƒç”¨ä¸€æ¬¡
        terminated = success
        truncated = self.current_step >= self.max_steps
        
        # è®°å½•æ€§èƒ½
        self.episode_rewards.append(reward)
        
        # ä¿¡æ¯å­—å…¸
        info = {
            'distance': self._get_distance_to_target(),
            'success': success,  # ğŸ†• ä½¿ç”¨å·²è®¡ç®—çš„success
            'episode_reward': sum(self.episode_rewards),
            'safety_penalty': safety_penalty,
        }
        
        # ğŸ†• ç›®æ ‡åˆ‡æ¢é€»è¾‘
        if terminated or truncated:
            if success:
                self.success_count += 1
                print(f"âœ… æˆåŠŸï¼è¿ç»­æˆåŠŸ {self.success_count}/{self.required_successes} æ¬¡")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ‡æ¢ç›®æ ‡
                if self.success_count >= self.required_successes:
                    if self.current_target_index < len(self.target_list) - 1:
                        # åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªç›®æ ‡
                        self.current_target_index += 1
                        self.success_count = 0
                        self.episodes_on_target = 0
                        
                        new_target = self.target_list[self.current_target_index]
                        print(f"ğŸ¯ ç›®æ ‡åˆ‡æ¢ï¼æ–°ç›®æ ‡ {self.current_target_index + 1}: {new_target}")
                    else:
                        print(f"ğŸ‰ æ‰€æœ‰ç›®æ ‡éƒ½æŒæ¡äº†ï¼å¯ä»¥å¼€å§‹éšæœºç›®æ ‡è®­ç»ƒ")
            else:
                # å¤±è´¥äº†ï¼Œé‡ç½®è¿ç»­æˆåŠŸè®¡æ•°
                if self.success_count > 0:
                    print(f"âŒ å¤±è´¥ï¼Œè¿ç»­æˆåŠŸè®¡æ•°é‡ç½®")
                self.success_count = 0
            
            # æ›´æ–°info
            info['consecutive_successes'] = self.success_count
            info['current_target_index'] = self.current_target_index
            info['episodes_on_target'] = self.episodes_on_target
        
        # æ›´æ–°å†å²æ•°æ®
        self.prev_distance = info['distance']
        self.prev_joint_positions = self._get_joint_positions()
        
        return observation, reward, terminated, truncated, info
        
    def _apply_safety_constraints_hard(self, action):
        """åº”ç”¨å®‰å…¨çº¦æŸ"""
        # é™åˆ¶åŠ¨ä½œå˜åŒ–ç‡
        if hasattr(self, 'prev_action'):
            
            max_change = 0.1  # æœ€å¤§åŠ¨ä½œå˜åŒ–
            action = np.clip(action, 
                            self.prev_action - max_change,
                            self.prev_action + max_change)
        self.prev_action = action.copy()
        
        return action
        
    def _apply_safety_constraints_easy(self, action):
        """åº”ç”¨å®‰å…¨çº¦æŸ"""
        # é™åˆ¶åŠ¨ä½œå˜åŒ–ç‡
        if hasattr(self, 'prev_action'):
            
            max_change = 0.3  # æœ€å¤§åŠ¨ä½œå˜åŒ–
            action = np.clip(action, 
                            self.prev_action - max_change,
                            self.prev_action + max_change)
        self.prev_action = action.copy()
        
        return action
    def _apply_torque_control(self, torques):
        """ç›´æ¥ä½¿ç”¨åŠ›çŸ©æ§åˆ¶å…³èŠ‚"""
        for i, (joint_idx, torque) in enumerate(zip(self.joint_indices, torques)):
            p.setJointMotorControl2(
                self.robot_id,
                joint_idx,
                controlMode=p.TORQUE_CONTROL,
                force=torque
            )
    def _action_to_torques(self, action):
        """å°†[-1,1]çš„åŠ¨ä½œæ˜ å°„åˆ°æ‰­çŸ©èŒƒå›´"""
        torques = []
        for i, joint_name in enumerate(self.joint_names):
            max_torque = self.joint_limits[joint_name]['effort']
            # ç›´æ¥æ˜ å°„åˆ°æ‰­çŸ©
            torque = action[i] * max_torque
            torques.append(torque)
        return torques

    def _action_to_joint_positions(self, action):
        """å°†åŠ¨ä½œè½¬æ¢ä¸ºå…³èŠ‚ä½ç½®"""
        target_positions = []
        
        for i, joint_name in enumerate(self.joint_names):
            limits = self.joint_limits[joint_name]
            
            # æ·»åŠ å®‰å…¨è¾¹è·
            low = limits['lower'] + self.joint_safety_margin
            high = limits['upper'] - self.joint_safety_margin
            
            # çº¿æ€§æ˜ å°„
            target_pos = low + (action[i] + 1) * (high - low) / 2
            target_positions.append(target_pos)
            
        return target_positions
    def _apply_joint_control(self, target_positions):
        """åº”ç”¨å…³èŠ‚æ§åˆ¶"""
        # if self.use_realistic_controller and self.realistic_controller is not None:
        #     # ä½¿ç”¨çœŸå®æ§åˆ¶å™¨
        #     return self.realistic_controller.apply_joint_control(target_positions = target_positions)
        # else:
        #     # ä½¿ç”¨åŸå§‹å…³èŠ‚æ§åˆ¶
        #     return self._apply_original_joint_control(target_positions)
        return self._apply_original_joint_control(target_positions)
    def _apply_original_joint_control(self, target_positions):
        # """åº”ç”¨å…³èŠ‚æ§åˆ¶"""
        # for i, (joint_idx, target_pos) in enumerate(zip(self.joint_indices, target_positions)):
        #     joint_name = self.joint_names[i]
        #     limits = self.joint_limits[joint_name]
            
        #     # è·å–å½“å‰å…³èŠ‚çŠ¶æ€
        #     joint_state = p.getJointState(self.robot_id, joint_idx)
        #     current_pos = joint_state[0]
        #     current_vel = joint_state[1]
            
        #     # é™åˆ¶é€Ÿåº¦
        #     max_vel = min(limits['velocity'], 
        #                  abs(target_pos - current_pos) * 10)
            
        #     # PDæ§åˆ¶
        #     p.setJointMotorControl2(
        #         self.robot_id,
        #         joint_idx,
        #         p.POSITION_CONTROL,
        #         targetPosition=target_pos,
        #         targetVelocity=0,
        #         force=limits['effort'] * 0.8,  # ä½¿ç”¨80%çš„æœ€å¤§åŠ›çŸ©
        #         maxVelocity=max_vel,
        #         positionGain=0.2,
        #         velocityGain=0.1
        #     )
        """åº”ç”¨å…³èŠ‚æ§åˆ¶ï¼šæ‰€æœ‰5ä¸ªå…³èŠ‚éƒ½ä½¿ç”¨PIDæ§åˆ¶ï¼ˆåŸºäºalpha_controller.pyçš„å‚æ•°ï¼‰"""
        
        # åˆå§‹åŒ–ï¼šåªåœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶åˆ›å»ºç§¯åˆ†è¯¯å·®å’Œå‰æ¬¡è¯¯å·®ç¼“å­˜
        if not hasattr(self, 'integral_errors'):
            # ä¸ºæ‰€æœ‰5ä¸ªå…³èŠ‚åˆ›å»ºPIDå†å²
            self.integral_errors = [0.0] * len(self.joint_indices)
            self.prev_errors = [0.0] * len(self.joint_indices)

        # æ§åˆ¶å‘¨æœŸ dtï¼ˆä¸ä»¿çœŸæ­¥é•¿æˆ– controller æ›´æ–°ç‡ä¸€è‡´ï¼‰
        dt = 1.0 / 240.0  # 240 Hz

        # æ‰€æœ‰5ä¸ªå…³èŠ‚çš„ PID å‚æ•°ï¼ˆæ¥è‡ªalpha_controller.pyçš„velocity_pid_paramsï¼‰
        p_gains = [0.5, 0.3, 0.15, 0.001, 10.0]        # P å¢ç›Š
        i_gains = [0.25, 0.25, 0.15, 0.015, 1.0]       # I å¢ç›Š
        d_gains = [0.005, 0.005, 0.001, 0.00001, 0.1]  # D å¢ç›Š
        integral_max = [1.0, 1.0, 1.0, 1.0, 0.5]       # ç§¯åˆ†é™å¹…ï¼ˆæœ«ç«¯æ‰§è¡Œå™¨ç”¨è¾ƒå°å€¼ï¼‰

        for i, (joint_idx, target_pos) in enumerate(zip(self.joint_indices, target_positions)):
            # è¯»å–å½“å‰çŠ¶æ€
            pos, vel, *_ = p.getJointState(self.robot_id, joint_idx)
            
            # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿ç´¢å¼•ä¸è¶Šç•Œ
            if i >= len(p_gains):
                print(f"è­¦å‘Šï¼šå…³èŠ‚ {i} è¶…å‡ºPIDå‚æ•°èŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
                kp, ki, kd, imax = 0.1, 0.01, 0.001, 1.0
            else:
                kp = p_gains[i]
                ki = i_gains[i]
                kd = d_gains[i]
                imax = integral_max[i]
            
            # è®¡ç®—ä½ç½®è¯¯å·®
            error = target_pos - pos
            
            # ç§¯åˆ†ç´¯åŠ å¹¶é™å¹…
            self.integral_errors[i] = max(-imax, min(self.integral_errors[i] + error * dt, imax))
            
            # å¾®åˆ†é¡¹
            d_error = (error - self.prev_errors[i]) / dt
            
            # PID è¾“å‡º
            torque = kp * error + ki * self.integral_errors[i] + kd * d_error
            
            # æ›´æ–°å†å²è¯¯å·®
            self.prev_errors[i] = error
            
            # é™åˆ¶æœ€å¤§æ‰­çŸ©
            if i < len(self.joint_names) and self.joint_names[i] in self.joint_limits:
                max_effort = self.joint_limits[self.joint_names[i]]['effort']
            else:
                max_effort = 50.0  # é»˜è®¤å€¼
                
            torque = max(-max_effort, min(torque, max_effort))

            # å‘é€æ‰­çŸ©æ§åˆ¶å‘½ä»¤
            p.setJointMotorControl2(
                bodyUniqueId=self.robot_id,
                jointIndex=joint_idx,
                controlMode=p.TORQUE_CONTROL,
                force=torque
            )

            # è°ƒè¯•ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œä»…æœ«ç«¯æ‰§è¡Œå™¨ï¼‰
            if i == 4:  # æœ«ç«¯æ‰§è¡Œå™¨
                if hasattr(self, '_debug_counter'):
                    self._debug_counter += 1
                else:
                    self._debug_counter = 0
                    
                # æ¯100æ­¥æ‰“å°ä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
                if self._debug_counter % 100 == 0:
                    print(f"æœ«ç«¯æ‰§è¡Œå™¨ - ç›®æ ‡: {target_pos:.4f}, å½“å‰: {pos:.4f}, "
                        f"è¯¯å·®: {error:.4f}, æ‰­çŸ©: {torque:.2f}")

    def set_eef_target(self, target_position):
        """è®¾ç½®æœ«ç«¯æ‰§è¡Œå™¨ç›®æ ‡ä½ç½®"""
        if self.realistic_controller is not None:
            self.realistic_controller.set_eef_target(target_position, enable=True)
            print(f"æœ«ç«¯æ‰§è¡Œå™¨ç›®æ ‡è®¾ç½®ä¸º: {target_position}")

    def toggle_control_mode(self, use_eef_control=False):
        """åˆ‡æ¢æ§åˆ¶æ¨¡å¼"""
        if self.realistic_controller is not None:
            self.realistic_controller.eef_control_enabled = use_eef_control
            if use_eef_control:
                print("å·²åˆ‡æ¢åˆ°æœ«ç«¯æ‰§è¡Œå™¨æ§åˆ¶æ¨¡å¼")
            else:
                print("å·²åˆ‡æ¢åˆ°å…³èŠ‚æ§åˆ¶æ¨¡å¼")

    def enable_controller_features(self, **kwargs):
        """å¯ç”¨æ§åˆ¶å™¨åŠŸèƒ½"""
        if self.realistic_controller is not None:
            self.realistic_controller.enable_features(**kwargs)

    def _check_safety(self):
        """æ£€æŸ¥å®‰å…¨çº¦æŸ"""
        penalty = 0.0
        
        # 1. æ£€æŸ¥è‡ªç¢°æ’
        if self._check_self_collision():
            penalty += 10.0
            self.collision_count += 1
            
        # # 2. æ£€æŸ¥å¥‡å¼‚æ€§
        # jacobian_cond = self._compute_jacobian_condition()
        # if jacobian_cond > 1 / self.singularity_threshold:
        #     penalty += 5.0 * (jacobian_cond * self.singularity_threshold - 1)
        #     self.singularity_count += 1
            
        # 3. æ£€æŸ¥å…³èŠ‚é™ä½
        joint_positions = self._get_joint_positions()
        for i, (pos, joint_name) in enumerate(zip(joint_positions, self.joint_names)):
            limits = self.joint_limits[joint_name]
            if pos < limits['lower'] or pos > limits['upper']:
                penalty += 5.0
                
        # 4. æ£€æŸ¥é€Ÿåº¦é™åˆ¶
        joint_velocities = self._get_joint_velocities()
        for i, (vel, joint_name) in enumerate(zip(joint_velocities, self.joint_names)):
            if abs(vel) > self.joint_limits[joint_name]['velocity']:
                penalty += 2.0
                
        return penalty
        
    def _check_self_collision(self):
        """æ£€æŸ¥è‡ªç¢°æ’"""
        for link1, link2 in self.link_pairs:
            contacts = p.getClosestPoints(
                self.robot_id, self.robot_id,
                self.collision_threshold,
                link1, link2
            )
            if contacts:
                return True
        return False
        
    def _compute_jacobian_condition(self):
        """è®¡ç®—é›…å¯æ¯”çŸ©é˜µæ¡ä»¶æ•°ï¼ˆå¥‡å¼‚æ€§åº¦é‡ï¼‰"""
        # è·å–é›…å¯æ¯”çŸ©é˜µ
        ee_state = p.getLinkState(self.robot_id, self.tcp_index, 
                                 computeForwardKinematics=True)
        ee_pos = ee_state[0]
        
        joint_positions = self._get_joint_positions()
        
        # ä½¿ç”¨PyBulletçš„é›…å¯æ¯”è®¡ç®—
        jacobian = p.calculateJacobian(
            self.robot_id,
            self.tcp_index,
            [0, 0, 0],  # å±€éƒ¨åæ ‡
            joint_positions,
            [0] * len(self.joint_indices),  # é›¶é€Ÿåº¦
            [0] * len(self.joint_indices)   # é›¶åŠ é€Ÿåº¦
        )
        
        # æå–çº¿æ€§éƒ¨åˆ†
        J_linear = np.array(jacobian[0])[:, :len(self.joint_indices)]
        
        # è®¡ç®—æ¡ä»¶æ•°
        try:
            cond = np.linalg.cond(J_linear)
        except:
            cond = 1e6  # å¥‡å¼‚æƒ…å†µ
            
        return cond
        
    def _get_observation(self):
        """è·å–è§‚å¯ŸçŠ¶æ€"""
        # å…³èŠ‚çŠ¶æ€
        joint_positions = self._get_joint_positions()
        joint_velocities = self._get_joint_velocities()
        joint_torques = self._get_joint_torques()
        
        # æœ«ç«¯æ‰§è¡Œå™¨çŠ¶æ€
        ee_state = p.getLinkState(self.robot_id, self.tcp_index,
                                 computeForwardKinematics=True)
        ee_pos = np.array(ee_state[0])
        ee_orn = np.array(ee_state[1])
        
        # ç›¸å¯¹ä¿¡æ¯
        relative_pos = self.target_position - ee_pos
        distance = self._get_distance_to_target()
        # é›…å¯æ¯”æ¡ä»¶æ•°ï¼ˆå½’ä¸€åŒ–ï¼‰
        # jacobian_cond = min(self._compute_jacobian_condition() / 100, 1.0)
        
        # ç»„åˆè§‚å¯Ÿ
        observation = np.concatenate([
            joint_positions,      # 5
            joint_velocities,     # 5
            ee_pos,              # 3
            #ee_orn,              # 4
            self.target_position, # 3
            #relative_pos,        # 3
            #[distance],         # 1
            #[jacobian_cond],     # 1
            #joint_torques        # 5
        ]).astype(np.float32)
        
        return observation
        
    def _get_joint_positions(self):
        """è·å–å…³èŠ‚ä½ç½®"""
        positions = []
        for joint_idx in self.joint_indices:
            joint_state = p.getJointState(self.robot_id, joint_idx)
            positions.append(joint_state[0])
        return np.array(positions)
        
    def _get_joint_velocities(self):
        """è·å–å…³èŠ‚é€Ÿåº¦"""
        velocities = []
        for joint_idx in self.joint_indices:
            joint_state = p.getJointState(self.robot_id, joint_idx)
            velocities.append(joint_state[1])
        return np.array(velocities)
        
    def _get_joint_torques(self):
        """è·å–å…³èŠ‚åŠ›çŸ©"""
        torques = []
        for joint_idx in self.joint_indices:
            joint_state = p.getJointState(self.robot_id, joint_idx)
            torques.append(joint_state[3])  # Applied joint motor torque
        return np.array(torques)
        
    def _calculate_reward(self):
        """è®¡ç®—å¥–åŠ±"""
        ee_pos = self._get_end_effector_pos()
        distance = np.linalg.norm(ee_pos - self.target_position)
        
        if self.dense_reward:
            # 1. è·ç¦»å¥–åŠ±ï¼ˆæŒ‡æ•°è¡°å‡ï¼‰
            distance_reward = np.exp(-10 * distance)
            
            if self.prev_distance is not None:
                # 2. è¿›æ­¥å¥–åŠ±
                progress = self.prev_distance - distance
                progress_reward = 50 * progress
            else:
                progress_reward = 0
            # 3. æˆåŠŸå¥–åŠ±
            if distance < 0.02:
                success_reward = 200
            elif distance < 0.05:
                success_reward = 50
            else:
                success_reward = 0
            
            # æ€»å¥–åŠ±
            reward = (distance_reward + progress_reward + success_reward)
            
        else:
            # ç¨€ç–å¥–åŠ±
            if distance < 0.02:
                reward = 100
            else:
                reward = -1
                
        return reward

        # """æç®€å¥–åŠ±å‡½æ•°"""
        # ee_pos = self._get_end_effector_pos()
        # distance = np.linalg.norm(ee_pos - self.target_position)
        
        # # æ–¹æ³•1ï¼šç®€å•çº¿æ€§å¥–åŠ±
        # reward = 1.0 - distance  # è·ç¦»è¶Šè¿‘å¥–åŠ±è¶Šé«˜
        
        # # æ–¹æ³•2ï¼šæˆåŠŸ/å¤±è´¥å¥–åŠ±
        # if distance < 0.02:
        #     reward = 10.0  # æˆåŠŸ
        # else:
        #     reward = -0.1  # å°æƒ©ç½šé¼“åŠ±å°½å¿«å®Œæˆ
        
        # return reward
        
    def _get_end_effector_pos(self):
        """è·å–æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®"""
        ee_state = p.getLinkState(self.robot_id, self.tcp_index)
        return np.array(ee_state[0])
        
    def _get_distance_to_target(self):
        """è®¡ç®—åˆ°ç›®æ ‡çš„è·ç¦»"""
        ee_pos = self._get_end_effector_pos()
        return np.linalg.norm(ee_pos - self.target_position)
        
    def _is_success(self):
        """æ£€æŸ¥æ˜¯å¦æˆåŠŸ"""
        return self._get_distance_to_target() < 0.02
        
    def _is_position_reachable(self, position):
        """ç®€å•çš„å¯è¾¾æ€§æ£€æŸ¥"""
        # æ£€æŸ¥æ˜¯å¦åœ¨å·¥ä½œç©ºé—´å†…
        distance_from_base = np.linalg.norm(position[:2])
        if distance_from_base < self.MIN_REACH or distance_from_base > self.MAX_REACH:
            return False
            
        # é«˜åº¦æ£€æŸ¥
        if position[2] < 0.05 or position[2] > 0.4:
            return False
            
        return True
        
    def update_curriculum(self, success_rate):
        """æ›´æ–°è¯¾ç¨‹å­¦ä¹ éš¾åº¦"""
        if self.curriculum_learning:
            self.recent_success_rate = success_rate
            
            # æ ¹æ®æˆåŠŸç‡è°ƒæ•´éš¾åº¦
            if success_rate > self.success_threshold and self.difficulty_level < 1.0:
                self.difficulty_level = min(1.0, self.difficulty_level + 0.1)
                print(f"Curriculum: Difficulty increased to {self.difficulty_level:.2f}")
            elif success_rate < 0.5 and self.difficulty_level > 0.0:
                self.difficulty_level = max(0.0, self.difficulty_level - 0.05)
                print(f"Curriculum: Difficulty decreased to {self.difficulty_level:.2f}")
                
    def render(self):
        """æ¸²æŸ“ç¯å¢ƒ"""
        if self.render_mode == "human":
            # GUIæ¨¡å¼è‡ªåŠ¨æ¸²æŸ“
            pass
            
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        if hasattr(self, 'physics_client'):
            p.disconnect(self.physics_client)
            
    def get_info(self):
        """è·å–ç¯å¢ƒä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•å’Œåˆ†æï¼‰"""
        return {
            'joint_positions': self._get_joint_positions(),
            'joint_velocities': self._get_joint_velocities(),
            'joint_torques': self._get_joint_torques(),
            'end_effector_pos': self._get_end_effector_pos(),
            'target_position': self.target_position,
            'distance': self._get_distance_to_target(),
            'jacobian_condition': self._compute_jacobian_condition(),
            'success_rate': self.success_count / max(1, self.current_step),
            'collision_rate': self.collision_count / max(1, self.current_step),
            'singularity_rate': self.singularity_count / max(1, self.current_step)
        }