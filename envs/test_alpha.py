import pybullet as p
import pybullet_data
import os
import time
import numpy as np
import gymnasium as gym
from gymnasium import spaces
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from collections import deque
import matplotlib.pyplot as plt

class AlphaRobotEnv(gym.Env):
    """Alpha Robot å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ"""
    
    def __init__(self, render_mode=None, max_steps=1000):
        super(AlphaRobotEnv, self).__init__()
        
        self.render_mode = render_mode
        self.max_steps = max_steps
        self.current_step = 0
        
        # åˆå§‹åŒ–ç›®æ ‡ä½ç½®ï¼ˆä¿®å¤ï¼šåœ¨åˆå§‹åŒ–æ—¶å°±è®¾ç½®ï¼‰
        self.target_position = np.array([0.2, 0.1, 0.2])
        
        # è®¾ç½®è·¯å¾„ - ä¿®å¤è·¯å¾„è§£æ
        current_dir = os.getcwd()  # è·å–å½“å‰å·¥ä½œç›®å½•
        self.urdf_path = os.path.join(current_dir, "alpha_description/urdf/alpha_robot.urdf")
        self.mesh_path = os.path.join(current_dir, "alpha_description/meshes")
        
        # æ‰“å°è·¯å¾„ç”¨äºè°ƒè¯•
        print(f"Current directory: {current_dir}")
        print(f"URDF path: {self.urdf_path}")
        print(f"Mesh path: {self.mesh_path}")
        
        # è¿æ¥PyBullet
        if self.render_mode == "human":
            self.physics_client = p.connect(p.GUI)
        else:
            self.physics_client = p.connect(p.DIRECT)
            
        p.setGravity(0, 0, -9.8)
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        p.setAdditionalSearchPath(self.mesh_path)
        p.setAdditionalSearchPath(os.path.dirname(self.urdf_path))
        
        # åˆå§‹åŒ–æœºå™¨äºº
        self._setup_robot()
        
        # å®šä¹‰åŠ¨ä½œå’Œè§‚å¯Ÿç©ºé—´
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, 
            shape=(len(self.movable_joints),), 
            dtype=np.float32
        )
        
        # è§‚å¯Ÿç©ºé—´ï¼šå…³èŠ‚ä½ç½®ã€é€Ÿåº¦ã€ç›®æ ‡ä½ç½®
        obs_dim = len(self.movable_joints) * 2 + 3 + 3  # å…³èŠ‚ä½ç½® + å…³èŠ‚é€Ÿåº¦ + æœ«ç«¯ä½ç½® + ç›®æ ‡ä½ç½®
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(obs_dim,), 
            dtype=np.float32
        )
        
        print(f"âœ… ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"   è§‚å¯Ÿç©ºé—´ç»´åº¦: {obs_dim}")
        print(f"   åŠ¨ä½œç©ºé—´ç»´åº¦: {len(self.movable_joints)}")
        
    def _setup_robot(self):
        """è®¾ç½®æœºå™¨äººå’Œç¯å¢ƒ"""
        # åˆ›å»ºåœ°é¢
        self.plane_id = p.createMultiBody(
            baseMass=0,
            baseCollisionShapeIndex=p.createCollisionShape(
                p.GEOM_PLANE, planeNormal=[0, 0, 1]
            ),
            basePosition=[0, 0, 0]
        )
        
        # åŠ è½½æœºå™¨äºº
        try:
            self.robot_id = p.loadURDF(
                self.urdf_path,
                basePosition=[0, 0, 0.1],
                useFixedBase=True,
                flags=p.URDF_USE_INERTIA_FROM_FILE | p.URDF_USE_SELF_COLLISION
            )
            print(f"âœ… æˆåŠŸåŠ è½½æœºå™¨äºº URDF: {self.urdf_path}")
        except Exception as e:
            print(f"âŒ åŠ è½½æœºå™¨äººå¤±è´¥: {e}")
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ–¹å—ä½œä¸ºæ›¿ä»£
            print("ä½¿ç”¨ç®€å•æ–¹å—ä½œä¸ºæ›¿ä»£æœºå™¨äºº...")
            self.robot_id = p.createMultiBody(
                baseMass=1.0,
                baseCollisionShapeIndex=p.createCollisionShape(
                    p.GEOM_BOX, halfExtents=[0.1, 0.1, 0.1]
                ),
                baseVisualShapeIndex=p.createVisualShape(
                    p.GEOM_BOX, halfExtents=[0.1, 0.1, 0.1], rgbaColor=[0, 1, 0, 1]
                ),
                basePosition=[0, 0, 0.1]
            )
        
        # è·å–å¯ç§»åŠ¨å…³èŠ‚
        num_joints = p.getNumJoints(self.robot_id)
        self.movable_joints = []
        self.joint_limits = []
        
        print(f"æœºå™¨äººå…³èŠ‚æ•°: {num_joints}")
        
        for i in range(num_joints):
            joint_info = p.getJointInfo(self.robot_id, i)
            joint_type = joint_info[2]
            joint_name = joint_info[1].decode('utf-8')
            
            print(f"å…³èŠ‚ {i}: {joint_name}, ç±»å‹: {joint_type}")
            
            if joint_type in [p.JOINT_REVOLUTE, p.JOINT_PRISMATIC]:
                self.movable_joints.append(i)
                lower_limit = joint_info[8]
                upper_limit = joint_info[9]
                self.joint_limits.append((lower_limit, upper_limit))
                print(f"  å¯ç§»åŠ¨å…³èŠ‚: {joint_name}, èŒƒå›´: [{lower_limit:.3f}, {upper_limit:.3f}]")
        
        # # å¦‚æœæ²¡æœ‰å¯ç§»åŠ¨å…³èŠ‚ï¼Œåˆ›å»ºä¸€äº›è™šæ‹Ÿå…³èŠ‚
        # if len(self.movable_joints) == 0:
        #     print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°å¯ç§»åŠ¨å…³èŠ‚ï¼Œä½¿ç”¨è™šæ‹Ÿå…³èŠ‚")
        #     self.movable_joints = [0, 1, 2]  # è™šæ‹Ÿå…³èŠ‚ID
        #     self.joint_limits = [(-1.0, 1.0), (-1.0, 1.0), (-1.0, 1.0)]
        
        print(f"å¯ç§»åŠ¨å…³èŠ‚æ•°: {len(self.movable_joints)}")
        
        # åˆ›å»ºç›®æ ‡å¯è§†åŒ–
        self.target_visual = p.createVisualShape(
            p.GEOM_SPHERE, radius=0.03, rgbaColor=[1, 0, 0, 0.8]
        )
        self.target_id = p.createMultiBody(
            baseMass=0,
            baseVisualShapeIndex=self.target_visual,
            basePosition=self.target_position
        )
        
    def reset(self, seed=None):
        """é‡ç½®ç¯å¢ƒ"""
        super().reset(seed=seed)
        
        self.current_step = 0
        
        # é‡ç½®æœºå™¨äººå…³èŠ‚åˆ°éšæœºä½ç½®
        for i, joint_id in enumerate(self.movable_joints):
            if i < len(self.joint_limits):
                lower, upper = self.joint_limits[i]
                random_pos = np.random.uniform(lower, upper)
                try:
                    p.resetJointState(self.robot_id, joint_id, random_pos)
                except:
                    # å¦‚æœæ˜¯è™šæ‹Ÿå…³èŠ‚ï¼Œè·³è¿‡
                    pass
        
        # è®¾ç½®æ–°çš„éšæœºç›®æ ‡ä½ç½®
        self.target_position = self.sample_valid_target()
        
        # æ›´æ–°ç›®æ ‡å¯è§†åŒ–
        try:
            p.resetBasePositionAndOrientation(
                self.target_id, 
                self.target_position, 
                [0, 0, 0, 1]
            )
        except:
            pass
        
        return self._get_observation(), {}
    
    def sample_valid_target(self,max_reach=0.4, min_radius=0.1, z_range=(0.1, 0.3)):
        while True:
            x = np.random.uniform(-max_reach, max_reach)
            y = np.random.uniform(-max_reach, max_reach)
            z = np.random.uniform(*z_range)
            r = np.linalg.norm([x, y, z])
            if min_radius < r <= max_reach:
                return np.array([x, y, z])
            
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        self.current_step += 1
        
        # åº”ç”¨åŠ¨ä½œåˆ°å…³èŠ‚
        for i, joint_id in enumerate(self.movable_joints):
            if i < len(self.joint_limits) and i < len(action):
                lower, upper = self.joint_limits[i]
                # å°†åŠ¨ä½œä»[-1,1]æ˜ å°„åˆ°å…³èŠ‚é™åˆ¶èŒƒå›´
                target_pos = lower + (action[i] + 1) * (upper - lower) / 2
                
                try:
                    p.setJointMotorControl2(
                        self.robot_id,
                        joint_id,
                        p.POSITION_CONTROL,
                        targetPosition=target_pos,
                        force=100
                    )
                except:
                    # å¦‚æœæ˜¯è™šæ‹Ÿå…³èŠ‚ï¼Œè·³è¿‡
                    pass
        
        # è¿è¡Œä»¿çœŸ
        p.stepSimulation()
        
        # è·å–è§‚å¯Ÿã€å¥–åŠ±ã€ç»ˆæ­¢æ¡ä»¶
        observation = self._get_observation()
        reward = self._calculate_reward()
        terminated = self._is_terminated()
        truncated = self.current_step >= self.max_steps
        
        return observation, reward, terminated, truncated, {}
    
    def _get_observation(self):
        """è·å–è§‚å¯ŸçŠ¶æ€"""
        joint_positions = []
        joint_velocities = []
        
        for i, joint_id in enumerate(self.movable_joints):
            if i < len(self.joint_limits):
                try:
                    joint_state = p.getJointState(self.robot_id, joint_id)
                    joint_positions.append(joint_state[0])
                    joint_velocities.append(joint_state[1])
                    # joint_state[0] â†’ å…³èŠ‚ä½ç½®  
                    # joint_state[1] â†’ å…³èŠ‚é€Ÿåº¦  
                    # joint_state[2] â†’ åä½œç”¨åŠ›  
                    # joint_state[3] â†’ å®é™…æ–½åŠ çš„åŠ›çŸ©
                except:
                    # å¦‚æœæ˜¯è™šæ‹Ÿå…³èŠ‚ï¼Œä½¿ç”¨éšæœºå€¼
                    joint_positions.append(0.0)
                    joint_velocities.append(0.0)
        
        # è·å–æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®ï¼ˆå‡è®¾æœ€åä¸€ä¸ªlinkæ˜¯æœ«ç«¯æ‰§è¡Œå™¨ï¼‰
        num_links = p.getNumJoints(self.robot_id)
        if num_links > 0:
            try:
                end_effector_state = p.getLinkState(self.robot_id, num_links - 1)
                end_effector_pos = end_effector_state[0]
            except:
                end_effector_pos = p.getBasePositionAndOrientation(self.robot_id)[0]
        else:
            end_effector_pos = p.getBasePositionAndOrientation(self.robot_id)[0]
        
        # ç¡®ä¿ç›®æ ‡ä½ç½®å·²åˆå§‹åŒ–
        if not hasattr(self, 'target_position') or self.target_position is None:
            self.target_position = np.array([0.2, 0.1, 0.2])
        
        # ç»„åˆè§‚å¯Ÿ - ç¡®ä¿ç»´åº¦ä¸€è‡´
        obs = np.concatenate([
            joint_positions,        # å…³èŠ‚ä½ç½®
            joint_velocities,       # å…³èŠ‚é€Ÿåº¦
            end_effector_pos,       # æœ«ç«¯æ‰§è¡Œå™¨ä½ç½® (3ç»´)
            self.target_position    # ç›®æ ‡ä½ç½® (3ç»´)
        ]).astype(np.float32)
        
        return obs
    def _calculate_reward(self):
        """è®¡ç®—å¯†é›†å¥–åŠ± (Dense Reward)"""
        # è·å–æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®
        num_links = p.getNumJoints(self.robot_id)
        if num_links > 0:
            try:
                end_effector_state = p.getLinkState(self.robot_id, num_links - 1)
                end_effector_pos = np.array(end_effector_state[0])
            except:
                end_effector_pos = np.array(p.getBasePositionAndOrientation(self.robot_id)[0])
        else:
            end_effector_pos = np.array(p.getBasePositionAndOrientation(self.robot_id)[0])
        
        # ç¡®ä¿ç›®æ ‡ä½ç½®å·²åˆå§‹åŒ–
        if not hasattr(self, 'target_position') or self.target_position is None:
            self.target_position = np.array([0.2, 0.1, 0.2])
        
        # è®¡ç®—åˆ°ç›®æ ‡çš„è·ç¦»
        distance = np.linalg.norm(end_effector_pos - self.target_position)
        
        # 1. è·ç¦»å¥–åŠ± - ä½¿ç”¨æŒ‡æ•°è¡°å‡å‡½æ•°ï¼Œæä¾›è¿ç»­çš„å¯†é›†åé¦ˆ
        max_distance = 0.4  # æœºæ¢°è‡‚çš„æœ€å¤§å¯è¾¾èŒƒå›´
        distance_reward = 20.0 * np.exp(-3.0 * distance / max_distance)
        
        # 2. è·ç¦»æ”¹è¿›å¥–åŠ± - å¥–åŠ±å‘ç›®æ ‡ç§»åŠ¨çš„è¡Œä¸º
        if hasattr(self, 'prev_distance'):
            distance_improvement = self.prev_distance - distance
            improvement_reward = 100.0 * distance_improvement  # åœ¨å°èŒƒå›´å†…æ”¾å¤§æ”¹è¿›å¥–åŠ±
        else:
            improvement_reward = 0.0
        self.prev_distance = distance
        
        # 3. ç›®æ ‡åˆ°è¾¾å¥–åŠ± - é«˜æ–¯å‡½æ•°å½¢å¼ï¼Œåœ¨ç›®æ ‡é™„è¿‘æä¾›å¼ºçƒˆå¥–åŠ±
        sigma = 0.02  # æ§åˆ¶å¥–åŠ±çš„é›†ä¸­ç¨‹åº¦ï¼Œé€‚åº”å°èŒƒå›´
        target_reward = 200.0 * np.exp(-0.5 * (distance / sigma) ** 2)
        
        # 4. åŠ¨ä½œå¹³æ»‘å¥–åŠ± - æƒ©ç½šå‰§çƒˆè¿åŠ¨
        joint_velocities = []
        for i, joint_id in enumerate(self.movable_joints):
            if i < len(self.joint_limits):
                try:
                    joint_state = p.getJointState(self.robot_id, joint_id)
                    joint_velocities.append(joint_state[1])
                except:
                    joint_velocities.append(0.0)
        
        if joint_velocities:
            # ä½¿ç”¨L2æ­£åˆ™åŒ–çš„å¹³æ»‘æƒ©ç½š
            velocity_penalty = -0.5 * np.sum(np.square(joint_velocities))
        else:
            velocity_penalty = 0.0
        
        # 5. æ–¹å‘å¥–åŠ± - å¥–åŠ±æœå‘ç›®æ ‡çš„è¿åŠ¨æ–¹å‘
        if hasattr(self, 'prev_end_effector_pos'):
            movement_vec = end_effector_pos - self.prev_end_effector_pos
            target_direction = self.target_position - end_effector_pos
            
            if np.linalg.norm(movement_vec) > 1e-6 and np.linalg.norm(target_direction) > 1e-6:
                # è®¡ç®—è¿åŠ¨æ–¹å‘ä¸ç›®æ ‡æ–¹å‘çš„ä½™å¼¦ç›¸ä¼¼åº¦
                cos_similarity = np.dot(movement_vec, target_direction) / (
                    np.linalg.norm(movement_vec) * np.linalg.norm(target_direction))
                direction_reward = 10.0 * cos_similarity  # å¢å¼ºæ–¹å‘å¥–åŠ±
            else:
                direction_reward = 0.0
        else:
            direction_reward = 0.0
        self.prev_end_effector_pos = end_effector_pos.copy()
        
        # 6. ç¨³å®šæ€§å¥–åŠ± - å¥–åŠ±åœ¨ç›®æ ‡é™„è¿‘ä¿æŒç¨³å®š
        if distance < 0.03:  # è°ƒæ•´ä¸ºæ›´å°çš„ç¨³å®šåŒºåŸŸ
            if joint_velocities:
                stability_reward = 5.0 * np.exp(-np.sum(np.square(joint_velocities)))
            else:
                stability_reward = 5.0
        else:
            stability_reward = 0.0
        
        # 7. ç²¾åº¦å¥–åŠ± - åœ¨éå¸¸æ¥è¿‘ç›®æ ‡æ—¶ç»™äºˆé¢å¤–å¥–åŠ±
        if distance < 0.01:
            precision_reward = 50.0
        elif distance < 0.02:
            precision_reward = 20.0
        else:
            precision_reward = 0.0
        
        # 8. æ—¶é—´æ•ˆç‡å¥–åŠ± - è½»å¾®æƒ©ç½šæ—¶é—´æ­¥é•¿ï¼Œé¼“åŠ±å¿«é€Ÿåˆ°è¾¾
        time_penalty = -0.02
        
        # ç»„åˆæ‰€æœ‰å¥–åŠ±åˆ†é‡
        total_reward = (distance_reward + 
                    improvement_reward + 
                    target_reward + 
                    velocity_penalty + 
                    direction_reward + 
                    stability_reward + 
                    precision_reward +
                    time_penalty)
        
        # å¯é€‰ï¼šæ·»åŠ è°ƒè¯•ä¿¡æ¯
        if hasattr(self, 'debug_reward') and self.debug_reward:
            print(f"Distance: {distance:.4f}, Distance Reward: {distance_reward:.4f}")
            print(f"Improvement Reward: {improvement_reward:.4f}")
            print(f"Target Reward: {target_reward:.4f}")
            print(f"Velocity Penalty: {velocity_penalty:.4f}")
            print(f"Direction Reward: {direction_reward:.4f}")
            print(f"Stability Reward: {stability_reward:.4f}")
            print(f"Precision Reward: {precision_reward:.4f}")
            print(f"Total Reward: {total_reward:.4f}")
        
        return total_reward
    
    def _is_terminated(self):
        """æ£€æŸ¥æ˜¯å¦ç»ˆæ­¢"""
        # è·å–æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®
        num_links = p.getNumJoints(self.robot_id)
        if num_links > 0:
            try:
                end_effector_state = p.getLinkState(self.robot_id, num_links - 1)
                end_effector_pos = np.array(end_effector_state[0])
            except:
                end_effector_pos = np.array(p.getBasePositionAndOrientation(self.robot_id)[0])
        else:
            end_effector_pos = np.array(p.getBasePositionAndOrientation(self.robot_id)[0])
        
        # ç¡®ä¿ç›®æ ‡ä½ç½®å·²åˆå§‹åŒ–
        if not hasattr(self, 'target_position') or self.target_position is None:
            self.target_position = np.array([0.2, 0.1, 0.2])
        
        # å¦‚æœæ¥è¿‘ç›®æ ‡ï¼Œä»»åŠ¡å®Œæˆ
        distance = np.linalg.norm(end_effector_pos - self.target_position)
        return distance < 0.02
    
    def render(self):
        """æ¸²æŸ“ç¯å¢ƒ"""
        if self.render_mode == "human":
            # GUIæ¨¡å¼å·²ç»åœ¨æ˜¾ç¤º
            pass
        
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        try:
            p.disconnect()
        except:
            pass

# ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´çš„Actor-Criticç½‘ç»œ
class ActorCritic(nn.Module):
    def __init__(self, state_size, action_size, hidden_size=256):
        super(ActorCritic, self).__init__()
        
        print(f"ğŸ§  åˆ›å»ºActorCriticç½‘ç»œ:")
        print(f"   è¾“å…¥ç»´åº¦: {state_size}")
        print(f"   è¾“å‡ºç»´åº¦: {action_size}")
        print(f"   éšè—å±‚ç»´åº¦: {hidden_size}")
        
        # å…±äº«å±‚
        self.shared = nn.Sequential(
            nn.Linear(state_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU()
        )
        
        # Actorç½‘ç»œï¼ˆè¾“å‡ºåŠ¨ä½œå‡å€¼ï¼‰
        self.actor_mean = nn.Linear(hidden_size, action_size)
        self.actor_std = nn.Linear(hidden_size, action_size)
        
        # Criticç½‘ç»œï¼ˆè¾“å‡ºçŠ¶æ€å€¼ï¼‰
        self.critic = nn.Linear(hidden_size, 1)
        
    def forward(self, x):
        # æ‰“å°è¾“å…¥ç»´åº¦ç”¨äºè°ƒè¯•
        # print(f"ActorCriticè¾“å…¥å½¢çŠ¶: {x.shape}")
        
        shared_out = self.shared(x)
        
        # Actorè¾“å‡º
        action_mean = torch.tanh(self.actor_mean(shared_out))
        action_std = F.softplus(self.actor_std(shared_out)) + 1e-3  # ä¿®å¤ï¼šä½¿ç”¨ F.softplus
        
        # Criticè¾“å‡º
        state_value = self.critic(shared_out)
        
        return action_mean, action_std, state_value


class PPOTrainer:
    """PPOè®­ç»ƒå™¨"""
    
    def __init__(self, env, state_size, action_size, lr=3e-4):
        self.env = env
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        print(f"ğŸ”§ PPOè®­ç»ƒå™¨é…ç½®:")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   çŠ¶æ€ç©ºé—´å¤§å°: {state_size}")
        print(f"   åŠ¨ä½œç©ºé—´å¤§å°: {action_size}")
        print(f"   å­¦ä¹ ç‡: {lr}")
        
        # åˆ›å»ºç½‘ç»œ
        self.policy = ActorCritic(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        
        # è¶…å‚æ•°
        self.gamma = 0.99
        self.gae_lambda = 0.95
        self.clip_epsilon = 0.2
        self.entropy_coeff = 0.01
        self.value_coeff = 0.5
        
        # å­˜å‚¨ç»éªŒ
        self.states = []
        self.actions = []
        self.rewards = []
        self.dones = []
        self.log_probs = []
        
    def collect_experience(self, num_steps):
        """æ”¶é›†ç»éªŒ"""
        try:
            state, _ = self.env.reset()
        except:
            # å¦‚æœé‡ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤çŠ¶æ€
            state = np.zeros(self.env.observation_space.shape[0])
        
        for step in range(num_steps):
            # æ‰“å°ç¬¬ä¸€æ­¥çš„çŠ¶æ€ç»´åº¦ç”¨äºè°ƒè¯•
            if step == 0:
                print(f"   ç¬¬ä¸€æ­¥çŠ¶æ€ç»´åº¦: {state.shape}")
            
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            with torch.no_grad():
                try:
                    action_mean, action_std, _ = self.policy(state_tensor)
                except Exception as e:
                    print(f"   ç½‘ç»œå‰å‘ä¼ æ’­é”™è¯¯: {e}")
                    print(f"   çŠ¶æ€å¼ é‡å½¢çŠ¶: {state_tensor.shape}")
                    print(f"   æœŸæœ›è¾“å…¥ç»´åº¦: {self.env.observation_space.shape[0]}")
                    raise e
                
            # ä»ç­–ç•¥ä¸­é‡‡æ ·åŠ¨ä½œ
            action_dist = torch.distributions.Normal(action_mean, action_std)
            action = action_dist.sample()
            log_prob = action_dist.log_prob(action).sum(dim=-1)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            try:
                next_state, reward, terminated, truncated, _ = self.env.step(action.cpu().numpy()[0])
                done = terminated or truncated
            except Exception as e:
                print(f"æ­¥éª¤æ‰§è¡Œé”™è¯¯: {e}")
                # ä½¿ç”¨é»˜è®¤å€¼
                next_state = np.zeros_like(state)
                reward = 0.0
                done = True
            
            # å­˜å‚¨ç»éªŒ
            self.states.append(state)
            self.actions.append(action.cpu().numpy()[0])
            self.rewards.append(reward)
            self.dones.append(done)
            self.log_probs.append(log_prob.cpu().numpy()[0])
            
            state = next_state
            
            if done:
                try:
                    state, _ = self.env.reset()
                except:
                    state = np.zeros(self.env.observation_space.shape[0])
    
    def train(self, num_epochs=10):
        """è®­ç»ƒç­–ç•¥"""
        if len(self.states) == 0:
            return
        
        # è½¬æ¢ä¸ºtensor
        states = torch.FloatTensor(self.states).to(self.device)
        actions = torch.FloatTensor(self.actions).to(self.device)
        old_log_probs = torch.FloatTensor(self.log_probs).to(self.device)
        
        # è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥
        advantages, returns = self._compute_advantages()
        
        for _ in range(num_epochs):
            # å‰å‘ä¼ æ’­
            action_mean, action_std, state_values = self.policy(states)
            
            # è®¡ç®—æ–°çš„logæ¦‚ç‡
            action_dist = torch.distributions.Normal(action_mean, action_std)
            new_log_probs = action_dist.log_prob(actions).sum(dim=-1)
            
            # è®¡ç®—ç­–ç•¥æ¯”ç‡
            ratio = torch.exp(new_log_probs - old_log_probs)
            
            # PPOæŸå¤±
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages
            policy_loss = -torch.min(surr1, surr2).mean()
            
            # ä»·å€¼å‡½æ•°æŸå¤±
            value_loss = nn.MSELoss()(state_values.squeeze(), returns)
            
            # ç†µæŸå¤±
            entropy = action_dist.entropy().mean()
            
            # æ€»æŸå¤±
            total_loss = policy_loss + self.value_coeff * value_loss - self.entropy_coeff * entropy
            
            # æ›´æ–°
            self.optimizer.zero_grad()
            total_loss.backward()
            self.optimizer.step()
        
        # æ¸…ç©ºç»éªŒ
        self.states.clear()
        self.actions.clear()
        self.rewards.clear()
        self.dones.clear()
        self.log_probs.clear()
    
    def _compute_advantages(self):
        """è®¡ç®—ä¼˜åŠ¿å’Œå›æŠ¥"""
        states = torch.FloatTensor(self.states).to(self.device)
        rewards = torch.FloatTensor(self.rewards).to(self.device)
        dones = torch.FloatTensor(self.dones).to(self.device)
        
        with torch.no_grad():
            _, _, values = self.policy(states)
            values = values.squeeze()
        
        advantages = []
        returns = []
        gae = 0
        
        for i in reversed(range(len(rewards))):
            if i == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[i + 1]
            
            delta = rewards[i] + self.gamma * next_value * (1 - dones[i]) - values[i]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[i]) * gae
            
            advantages.insert(0, gae)
            returns.insert(0, gae + values[i])
        
        advantages = torch.FloatTensor(advantages).to(self.device)
        returns = torch.FloatTensor(returns).to(self.device)
        
        # æ ‡å‡†åŒ–ä¼˜åŠ¿
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        return advantages, returns

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # åˆ›å»ºç¯å¢ƒ
    env = AlphaRobotEnv(render_mode="human")  # è®¾ç½®ä¸º"human"å¯ä»¥çœ‹åˆ°è®­ç»ƒè¿‡ç¨‹
    
    # è·å–çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´å¤§å°
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.shape[0]
    
    print(f"State size: {state_size}")
    print(f"Action size: {action_size}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = PPOTrainer(env, state_size, action_size)
    
    # è®­ç»ƒå‚æ•°
    num_iterations = 1000
    steps_per_iteration = 2048
    
    rewards_history = []
    
    print("Starting training...")
    
    for iteration in range(num_iterations):
        # æ”¶é›†ç»éªŒ
        trainer.collect_experience(steps_per_iteration)
        
        # è®­ç»ƒ
        trainer.train()
        
        # æµ‹è¯•å½“å‰ç­–ç•¥
        if iteration % 10 == 0:
            test_reward = test_policy(env, trainer.policy)
            rewards_history.append(test_reward)
            print(f"Iteration {iteration}, Test Reward: {test_reward:.2f}")
            
            # ä¿å­˜æ¨¡å‹
            torch.save(trainer.policy.state_dict(), f'alpha_robot_model_{iteration}.pth')
    
    env.close()
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(rewards_history)
    plt.title('Alpha Robot Training Progress')
    plt.xlabel('Iteration (x10)')
    plt.ylabel('Test Reward')
    plt.grid(True)
    plt.savefig('training_progress.png')
    plt.show()


def test_policy(env, policy, num_episodes=3):
    """æµ‹è¯•ç­–ç•¥"""
    total_reward = 0
    
    for _ in range(num_episodes):
        try:
            state, _ = env.reset()
        except:
            state = np.zeros(env.observation_space.shape[0])
        
        episode_reward = 0
        done = False
        steps = 0
        
        while not done and steps < 1000:
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            
            with torch.no_grad():
                action_mean, _, _ = policy(state_tensor)
            
            action = action_mean.cpu().numpy()[0]
            
            try:
                state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                episode_reward += reward
            except:
                done = True
                reward = 0
            
            steps += 1
        
        total_reward += episode_reward
    
    return total_reward / num_episodes


if __name__ == "__main__":
    main()